# relevant-document-retrieval

> **Oneâ€‘liner:** A lightweight, selfâ€‘hosted RAG layer that turns *naturalâ€‘language prompts* into **precise document hits** using local, pretrained embedding models and classic lexical search.

---

## Table of Contents

1. [Project Overview](#project-overview)
2. [Key Features](#key-features)
3. [Architecture](#architecture)
4. [Quick Start](#quick-start)
5. [Models](#models)
6. [Directory & File Reference](#directory--file-reference)
7. [Configuration](#configuration)
8. [Testing & Linting](#testing--linting)
9. [Benchmarking](#benchmarking)
10. [Deployment](#deployment)
11. [Contributing](#contributing)
12. [Roadmap](#roadmap)
13. [License](#license)
14. [Authors & Acknowledgements](#authors--acknowledgements)
15. [FAQ / Troubleshooting](#faq--troubleshooting)

---

## Project Overview

Modern LLM agents excel at *reasoning* but still struggle with *factual recall* beyond their training cutâ€‘off. **relevantâ€‘documentâ€‘retrieval** fixes this by adding a retrievalâ€‘augmented generation (RAG) layer: it ingests arbitrary files (PDF, Markdown, HTML, etc.), creates dense vector embeddings plus BM25 indices, and exposes API & CLI endpoints that return the *most relevant* chunks for a given question.

Key design goals:

* **Localâ€‘first.** No cloud keys, no external calls â€” everything runs on your machine or server.
* **Hackathonâ€‘friendly.** `make docker-up`, drop docs into *samples/*, query. Thatâ€™s it.
* **Composable.** Clear extension points for new chunkers, parsers, and embedding backâ€‘ends.

---

## Key Features

* ğŸ” **Hybrid Search:** dense vectors (`pgvector`) + [BM25](https://en.wikipedia.org/wiki/Okapi_BM25) scoring for high recall & precision.
* âœ‚ï¸ **MMR Chunking:** onâ€‘theâ€‘fly [MaximalÂ MarginalÂ Relevance](https://huggingface.co/docs/transformers/main/en/main_classes/retriever#maximal-marginal-relevance) reduces redundancy while preserving coverage.
* âš¡ **Streaming API:** Serverâ€‘Sent Events (SSE) let frontâ€‘ends display retrieval hits in real time.
* ğŸ§© **Retrievalâ€‘QA Chain:** readyâ€‘toâ€‘use LangChain `RetrievalQA` wrapper to feed results directly into any LLM.
* ğŸ¤ **Pretrained, Offline Models:** ships with `sentence-transformers/all-MiniLM-L6-v2` (open licence, 384â€‘dim vectors) â€” no external API keys.
* ğŸ³ **Oneâ€‘command bootâ€‘up:** `make docker-up` spins PostgresÂ 15 + pgvectorÂ 0.7 and the FastAPI service.

---

## Architecture

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    ingest        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Files    â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   Ingestor   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚ chunks + embeddings
                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                           â”‚   Postgres + pgvectorâ”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚ topâ€‘k vectors
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  query   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ REST / CLI /  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Retrieval Service    â”‚
â”‚   LangChain   â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚ docs
                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                           â”‚  Retrievalâ€‘QA Chain  â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â–¼
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚  Downâ€‘stream LLM   â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```                         

### Components

| Piece                  | Role                                                            |
| ---------------------- | --------------------------------------------------------------- |
| **FastAPI** gateway    | Exposes `/ingest` and `/query` endpoints (SSEâ€‘capable).         |
| **Ingestor workers**   | Extract text (PDFMiner, BeautifulSoup), split, embed.           |
| **Vector store**       | PostgresÂ 15 + [pgvector](https://github.com/pgvector/pgvector). |
| **Retrievalâ€‘QA Chain** | LangChain wrapper that combines retriever + LLM.                |
| **Embedding model**    | `sentence-transformers/all-MiniLM-L6-v2` (local).               |

---

## Quick Start

### Prerequisites

* **PythonÂ â‰¥Â 3.11** (for CLI; the main service runs in Docker)
* **DockerÂ â‰¥Â 24.0** and **dockerâ€‘composeÂ v2**
* CPU is enough for small demos; GPU (CUDAÂ 11+) speeds up embeddings.

### Local Setup

```bash
git clone https://github.com/your-org/relevant-document-retrieval.git
cd relevant-document-retrieval

python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

# Copy and edit env vars
cp .env.example .env
```

### Running with Docker Compose

```bash
# 1) Start Postgres + pgvector + API
docker compose up -d --build   # or: make docker-up

# 2) Ingest sample docs
rel-doc ingest ./samples

# 3) Query
rel-doc query "Ğ§Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğµ ĞºĞ¾Ğ³ĞµÑ€ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾Ğ»Ğ½?"
```

### Makefile goodies

| Command            | Action                             |
| ------------------ | ---------------------------------- |
| `make docker-up`   | Build & start all containers       |
| `make docker-down` | Stop + remove containers & volumes |
| `make fmt`         | Run *ruff* + isort                 |
| `make test`        | Run unit tests inside tox          |

---

## Models

| Model                                    | Dim | License    | Notes                                       |
| ---------------------------------------- | --- | ---------- | ------------------------------------------- |
| `sentence-transformers/all-MiniLM-L6-v2` | 384 | Apacheâ€‘2.0 | Default embedder; <50â€¯MB.                   |
| *Bring your own*                         | â€”   | â€”          | Any model accepted by LangChain Embeddings. |

> **No datasets included.** The system indexes whatever files you drop into *samples/* or pass to the ingest API.

---

## Directory & File Reference

<details>
<summary>Firstâ€‘level tree</summary>

```text
./
â”œâ”€â”€ __pycache__/
â”œâ”€â”€ __run_once_flags__/
â”œâ”€â”€ database/
â”œâ”€â”€ document_load_pipe.py
â”œâ”€â”€ init_stat.txt
â”œâ”€â”€ llm/
â”œâ”€â”€ main.py
â”œâ”€â”€ metadata_structure.md
â”œâ”€â”€ models/
â”œâ”€â”€ parsers/
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ tempCodeRunnerFile.py
â”œâ”€â”€ test/
â””â”€â”€ utils/
```

</details>

| Path                                               | Purpose                                  | Key tech / notes                                                                   |
| -------------------------------------------------- | ---------------------------------------- | ---------------------------------------------------------------------------------- |
| [`main.py`](./main.py)                             | FastAPI app entrypoint                   | [`FastAPI`](https://fastapi.tiangolo.com/) Â· [`uvicorn`](https://www.uvicorn.org/) |
| [`document_load_pipe.py`](./document_load_pipe.py) | CLI for batch ingestion                  | [`click`](https://click.palletsprojects.com/)                                      |
| [`llm/`](./llm/)                                   | Provider adapters (HF, local)            | LangChain wrappers                                                                 |
| [`models/`](./models/)                             | Pydantic DTOs / ORM models               | [`SQLModel`](https://sqlmodel.tiangolo.com/)                                       |
| [`parsers/`](./parsers/)                           | Fileâ€‘type handlers (PDF, HTML, Markdown) | PDFMiner, BeautifulSoup                                                            |
| [`utils/`](./utils/)                               | Common helpers (chunking, hashing)       | pure Python                                                                        |
| [`database/`](./database/)                         | SQL migrations, seeds                    | Alembic scripts                                                                    |
| [`test/`](./test/)                                 | Pytest suites & fixtures                 | [`pytest`](https://docs.pytest.org/)                                               |

---

## Configuration

| Variable            | Default                                  | Description                                      |
| ------------------- | ---------------------------------------- | ------------------------------------------------ |
| `POSTGRES_HOST`     | `localhost`                              | DB endpoint                                      |
| `POSTGRES_PORT`     | `5432`                                   | â€”                                                |
| `POSTGRES_USER`     | `postgres`                               | â€”                                                |
| `POSTGRES_PASSWORD` | `postgres`                               | â€”                                                |
| `EMBEDDING_MODEL`   | `sentence-transformers/all-MiniLM-L6-v2` | Any embedding model string accepted by LangChain |
| `CHUNK_SIZE`        | `512`                                    | Tokens per chunk                                 |
| `MMR_K`             | `20`                                     | Window size for MMR selection                    |
| `TOP_K`             | `5`                                      | Retrieval depth                                  |

Create your own `.env` or pass vars via `docker compose --env-file`.

---

## Testing & Linting

```bash
pytest -q        # unit & integration
ruff check .     # static analysis
ruff format .    # autoâ€‘format
```

Preâ€‘commit hooks live in `.pre-commit-config.yaml`; run `pre-commit install` after cloning.

---

## Benchmarking

| Metric                       | Script                    | Notes                 |
| ---------------------------- | ------------------------- | --------------------- |
| Ingest throughput (docs/sec) | `scripts/bench_ingest.py` | CPU i7â€‘12700H         |
| Query latency (P99)          | `scripts/bench_query.py`  | 100 parallel requests |

CSV outputs live in `bench/` and can be plotted with `python scripts/plot.py`.

---

## Deployment

| Target             | Hint                                                                                 |
| ------------------ | ------------------------------------------------------------------------------------ |
| **Docker Compose** | `docker compose -f deploy/prod.yml up -d --scale api=3`                              |
| **Kubernetes**     | See `charts/reldoc/values.yaml` for resources & autoscaling                          |
| **HF Spaces**      | Comment out Postgres, switch to `sqlite+aiosqlite://`                                |
| **GPU box**        | Mount `--gpus all` and set `EMBEDDING_MODEL=sentence-transformers/all-mpnet-base-v2` |

Tune `shared_buffers` & `work_mem` in `postgresql.conf` for large corpora.

---

## Contributing

We follow **[Conventional Commits](https://www.conventionalcommits.org/)** + GitHub Flow.

1. `feat/my-brief-topic` branch off **main**
2. `make fmt && make test` must pass
3. Open PR, request review from `@team-Budapest/owners`

All code changes require matching unit tests.

---

## Roadmap

* [ ] PDF table extraction
* [ ] Vectorâ€‘aware summarization endpoint
* [ ] Web UI (React + [Intersection Observer](https://developer.mozilla.org/docs/Web/API/Intersection_Observer) for infinite scroll)
* [ ] Model hotâ€‘swap via OCI image
* [ ] Bench suite on A100 vs CPU

---

## License

This project is licensed under the **Apache-2.0** License â€“ see [LICENSE](./LICENSE) for details.

---

## Authors & Acknowledgements

Developed with â˜• by **ĞšĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ° Â«Ğ‘ÑƒĞ´Ğ°Ğ¿ĞµÑˆÑ‚Â»**.
Big thanks to the OSS community behind FastAPI, pgvector, SentenceTransformers, and LangChain.

---

## FAQ / Troubleshooting

| Question                                                     | Fix                                                                                 |
| ------------------------------------------------------------ | ----------------------------------------------------------------------------------- |
| **`psycopg2.OperationalError: could not connect to server`** | Ensure `docker compose ps` shows the `db` container healthy; check `POSTGRES_HOST`. |
| **`pgvector extension "vector" does not exist`**             | Run `CREATE EXTENSION IF NOT EXISTS vector;` or rebuild with `make docker-up`.      |
| CUDAâ€‘enabled embedder OOM                                    | Lower `BATCH_SIZE` or switch to CPU model.                                          |
| Tokenizer mismatch error                                     | Delete `vector_cache/`, re-ingest with consistent model/version.                    |
| CORS blocked in browser                                      | Set `ALLOWED_ORIGINS=*` (dev) or whitelist domains.                                 |
| UnicodeDecodeError on ingest                                 | Add `--encoding utf-8` flag or update `parsers/file_loader.py`.                     |
| CLI hangs on Windows                                         | Use `winpty rel-doc ...` (Git Bash) or WSL.                                         |
| Inaccurate search results                                    | Increase \`CH                                                                       |
